# Coursera-Machine-Learning-by-Stanford-University
Coursera-Machine Learning by Stanford University

# WEEK 1

## 1.1 Introduction




Welcome to Machine Learning! In this module, we introduce the core idea of teaching a computer to learn concepts using data—without being explicitly programmed. 

The Course Wiki is under construction. Please visit the resources tab for the most complete and up-to-date information.

- Welcome
- What is Machine Learning?
- Supervised Learning
- Unsupervised Learning

### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%201/Lecture1.pdf

## 1.2 Linear Regression with One Variable

Linear regression predicts a real-valued output based on an input value. 

We discuss the application of linear regression to housing price prediction, present the notion of a cost function, and introduce the gradient descent method for learning.


### Model and Cost Function
- Model Representation
- Cost Function
- Cost Function - Intuition I
- Cost Function - Intuition II

### Parameter Learning
- Gradient Descent
- Gradient Descent Intuition
- Gradient Descent For Linear Regression

### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%201/Lecture2.pdf





## 1.3 Linear Algebra Review

This optional module provides a refresher on linear algebra concepts. 

Basic understanding of linear algebra is necessary for the rest of the course, especially as we begin to cover models with multiple variables.


- Matrices and Vectors
- Addition and Scalar Multiplication
- Matrix Vector Multiplication
- Matrix Matrix Multiplication
- Matrix Multiplication Properties
- Inverse and Transpose

### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%201/Lecture3.pdf


# WEEK 2

## 2.1 Environment Setup Instructions


## 2.2 Linear Regression with Multiple Variables
### Multivariate Linear Regression

What if your input has more than one value? In this module, we show how linear regression can be extended to accommodate multiple input features. 

We also discuss best practices for implementing linear regression.





### Computing Parameters Analitically

- Normal Equation
- Normal Equation Noninvertibility

### Submitting Programming Assignments 


- Working on and Submitting Programming Assignments

### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%202/Lecture4.pdf

## 2.2 Octave/Matlab Tutorial

This course includes programming assignments designed to help you understand how to implement the learning algorithms in practice. 

To complete the programming assignments, you will need to use Octave or MATLAB. 

This module introduces Octave/Matlab and shows you how to submit an assignment.

- Basic Operations
- Moving Data Around
- Computing on Data
- Plotting Data
- Control Statements: for, while, if statement
- Vectorization


### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%202/Lecture5.pdf


# WEEK 3

## 3.1 Logistic Regression
Logistic regression is a method for classifying data into discrete outcomes. 

For example, we might use logistic regression to classify an email as spam or not spam. 

In this module, we introduce the notion of classification, the cost function for logistic regression, and the application of logistic regression to multi-class classification.

### Classification and Representation
- Classification
- Hypothesis Representation
- Decision Boundary

### Logistic Regression Model
- Cost Function
- Simplified Cost Function and Gradient Descent
- Advanced Optimization

### Multiclass Classification
- Multiclass Classification: One-vs-all


### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%203/Lecture6.pdf


## 3.2 Regularization
Machine learning models need to generalize well to new examples that the model has not seen in practice. 

In this module, we introduce regularization, which helps prevent models from overfitting the training data.


### Solving the Problem of Overfitting
- The Problem of Overfitting
- Cost Function
- Regularized Linear Regression
- Regularized Logistic Regression


### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%203/Lecture7.pdf



# WEEK 4
## 4.1 Neural Networks: Representation
Neural networks is a model inspired by how the brain works. 

It is widely used today in many applications: when your phone interprets and understand your voice commands, it is likely that a neural network is helping to understand your speech; when you cash a check, the machines that automatically read the digits also use neural networks.

### Motivations
- Non-linear Hypotheses
- Neurons and the Brain


### Neural Networks
- Model Representation I
- Model Representation II


### Applications
- Examples and Intuitions I
- Examples and Intuitions II
- Multiclass Classification

### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%204/Lecture8.pdf



# WEEK 5

## 5.1 Neural Networks: Learning
In this module, we introduce the backpropagation algorithm that is used to help learn parameters for a neural network. 

At the end of this module, you will be implementing your own neural network for digit recognition.

### Cost Function and Backpropagation

- Cost Function
- Backpropagation Algorithm
- Backpropagation Intuition

### Backpropagation in Practice


- Implementation Note: Unrolling Parameters
- Gradient Checking

- Random Initialization
- Putting It Together

### Application of Neural Networks
- Autonomous Driving

### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%205/Lecture9.pdf



# WEEK 6

## 6.1 Advice for Applying Machine Learning
Applying machine learning in practice is not always straightforward. 

In this module, we share best practices for applying machine learning in practice, and discuss the best ways to evaluate performance of the learned models.


### Evaluating a Learning Algorithm
- Deciding What to Try Next
- Evaluating a Hypothesis
- Model Selection and Train/Validation/Test Sets

### Bias vs. Variance
- Diagnosing Bias vs. Variance
- Regularization and Bias/Variance
- Learning Curves
- Deciding What to Do Next Revisited





### Slide:

https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%206/Lecture10.pdf

## 6.2 Machine Learning System Design
To optimize a machine learning algorithm, you’ll need to first understand where the biggest improvements can be made. 

In this module, we discuss how to understand the performance of a machine learning system with multiple parts, and also how to deal with skewed data.

### Buildinf a Spam Classifier
- Prioritizing What to Work On
- Error Analysis



### Handling Skewed Data
- Error Metrics for Skewed Classes
- Trading Off Precision and Recall

### Using Large Data Sets
- Data For Machine Learning


### Slide:
https://github.com/grapestone5321/Coursera-Machine-Learning-by-Stanford-University/blob/master/Slides/Week%206/Lecture11.pdf


# WEEK 7

## Support Vector Machines

Support vector machines, or SVMs, is a machine learning algorithm for classification. 

We introduce the idea and intuitions behind SVMs and discuss how to use it in practice.

# WEEK 8

# WEEK 9

# WEEK 10

# WEEK 11

